# -*- coding: utf-8 -*-
"""Hybrid ConvNeXtâ€“Swin U-Net.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DMXRU3sg52HO_0mW-ELoNCLDcl4GWF6e
"""

from google.colab import drive
drive.mount('/content/drive')

import os
import zipfile

ZIP_PATH = "/content/drive/MyDrive/brisc2025.zip"
EXTRACT_ROOT = "/content/datasets"

os.makedirs(EXTRACT_ROOT, exist_ok=True)

with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:
    zip_ref.extractall(EXTRACT_ROOT)

print("âœ… ZIP extracted successfully")

# Find the BRISC root directory dynamically
def find_brisc_root(base_dir):
    for root, dirs, files in os.walk(base_dir):
        if "classification_task" in dirs and "segmentation_task" in dirs:
            return root
    return None

BRISC_ROOT = find_brisc_root(EXTRACT_ROOT)

if BRISC_ROOT is None:
    raise RuntimeError("âŒ BRISC dataset root not found")

print(f"âœ… BRISC root detected at: {BRISC_ROOT}")

CLASSIFICATION_DIR = os.path.join(BRISC_ROOT, "classification_task")
SEGMENTATION_DIR = os.path.join(BRISC_ROOT, "segmentation_task")

SEG_IMAGES_DIR = os.path.join(SEGMENTATION_DIR, "images")
SEG_MASKS_DIR  = os.path.join(SEGMENTATION_DIR, "masks")

import os

print("ðŸ“ BRISC ROOT:")
print(os.listdir(BRISC_ROOT))

print("\nðŸ“ segmentation_task contents:")
print(os.listdir(SEGMENTATION_DIR))

import os

print("ðŸ“ segmentation_task/train:")
print(os.listdir(os.path.join(SEGMENTATION_DIR, "train")))

print("\nðŸ“ segmentation_task/test:")
print(os.listdir(os.path.join(SEGMENTATION_DIR, "test")))

SEG_TRAIN_DIR = os.path.join(SEGMENTATION_DIR, "train")
SEG_TEST_DIR  = os.path.join(SEGMENTATION_DIR, "test")

TRAIN_IMAGES_DIR = os.path.join(SEG_TRAIN_DIR, "images")
TRAIN_MASKS_DIR  = os.path.join(SEG_TRAIN_DIR, "masks")

TEST_IMAGES_DIR  = os.path.join(SEG_TEST_DIR, "images")
TEST_MASKS_DIR   = os.path.join(SEG_TEST_DIR, "masks")

assert os.path.isdir(TRAIN_IMAGES_DIR), "âŒ train/images missing"
assert os.path.isdir(TRAIN_MASKS_DIR),  "âŒ train/masks missing"
assert os.path.isdir(TEST_IMAGES_DIR),  "âŒ test/images missing"
assert os.path.isdir(TEST_MASKS_DIR),   "âŒ test/masks missing"

print("âœ… BRISC train/test image & mask folders verified")

print("Train images:", len(os.listdir(TRAIN_IMAGES_DIR)))
print("Train masks :", len(os.listdir(TRAIN_MASKS_DIR)))
print("Test images :", len(os.listdir(TEST_IMAGES_DIR)))
print("Test masks  :", len(os.listdir(TEST_MASKS_DIR)))

!pip install opencv-python albumentations

import albumentations as A
from albumentations.pytorch import ToTensorV2

IMAGE_SIZE = 256

train_transform = A.Compose([
    A.Resize(IMAGE_SIZE, IMAGE_SIZE),
    A.HorizontalFlip(p=0.5),   # light augmentation
    A.VerticalFlip(p=0.5),
    A.Normalize(mean=(0.5, 0.5, 0.5),
                std=(0.5, 0.5, 0.5)),
    ToTensorV2()
])

test_transform = A.Compose([
    A.Resize(IMAGE_SIZE, IMAGE_SIZE),
    A.Normalize(mean=(0.5, 0.5, 0.5),
                std=(0.5, 0.5, 0.5)),
    ToTensorV2()
])

import cv2
import torch
from torch.utils.data import Dataset
import os

class BRISCSegmentationDataset(Dataset):
    def __init__(self, images_dir, masks_dir, transform=None):
        self.images_dir = images_dir
        self.masks_dir = masks_dir
        self.transform = transform

        self.images = sorted(os.listdir(images_dir))
        self.masks  = sorted(os.listdir(masks_dir))

        assert len(self.images) == len(self.masks), "Imageâ€“mask mismatch"

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        img_path  = os.path.join(self.images_dir, self.images[idx])
        mask_path = os.path.join(self.masks_dir, self.masks[idx])

        image = cv2.imread(img_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)
        mask = (mask > 0).astype("float32")

        if self.transform:
            augmented = self.transform(image=image, mask=mask)
            image = augmented["image"]
            mask  = augmented["mask"].unsqueeze(0)

        return image, mask

train_dataset = BRISCSegmentationDataset(
    TRAIN_IMAGES_DIR,
    TRAIN_MASKS_DIR,
    transform=train_transform
)

test_dataset = BRISCSegmentationDataset(
    TEST_IMAGES_DIR,
    TEST_MASKS_DIR,
    transform=test_transform
)

print("Train:", len(train_dataset))
print("Test :", len(test_dataset))

import matplotlib.pyplot as plt

img, mask = train_dataset[0]

plt.figure(figsize=(8,4))
plt.subplot(1,2,1)
plt.imshow(img.permute(1,2,0))
plt.title("Preprocessed Image")
plt.axis("off")

plt.subplot(1,2,2)
plt.imshow(mask.squeeze(), cmap="gray")
plt.title("Preprocessed Mask")
plt.axis("off")

plt.show()

!pip install segmentation-models-pytorch torchmetrics

import torch
import torch.nn as nn
import numpy as np
from torch.utils.data import DataLoader
from tqdm import tqdm
import segmentation_models_pytorch as smp

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

BATCH_SIZE = 8
NUM_WORKERS = 2

train_loader = DataLoader(
    train_dataset,
    batch_size=BATCH_SIZE,
    shuffle=True,
    num_workers=NUM_WORKERS,
    pin_memory=True
)

test_loader = DataLoader(
    test_dataset,
    batch_size=BATCH_SIZE,
    shuffle=False,
    num_workers=NUM_WORKERS,
    pin_memory=True
)

model = smp.Unet(
    encoder_name="resnet34",
    encoder_weights="imagenet",
    in_channels=3,
    classes=1
).to(device)

loss_fn = smp.losses.DiceLoss(mode="binary")
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

def dice_score(preds, targets, smooth=1e-6):
    preds = preds.contiguous()
    targets = targets.contiguous()

    intersection = (preds * targets).sum(dim=(2,3))
    union = preds.sum(dim=(2,3)) + targets.sum(dim=(2,3))

    dice = (2. * intersection + smooth) / (union + smooth)
    return dice.mean()


def iou_score(preds, targets, smooth=1e-6):
    intersection = (preds * targets).sum(dim=(2,3))
    total = preds.sum(dim=(2,3)) + targets.sum(dim=(2,3))
    union = total - intersection

    iou = (intersection + smooth) / (union + smooth)
    return iou.mean()

def train_one_epoch(model, loader, optimizer, loss_fn):
    model.train()
    epoch_loss = 0.0

    for images, masks in tqdm(loader):
        images = images.to(device)
        masks = masks.to(device)

        optimizer.zero_grad()
        outputs = model(images)
        loss = loss_fn(outputs, masks)
        loss.backward()
        optimizer.step()

        epoch_loss += loss.item()

    return epoch_loss / len(loader)

def evaluate(model, loader):
    model.eval()
    dice_scores = []
    iou_scores = []

    with torch.no_grad():
        for images, masks in loader:
            images = images.to(device)
            masks = masks.to(device)

            outputs = model(images)
            preds = torch.sigmoid(outputs)
            preds = (preds > 0.5).float()

            d = dice_score(preds, masks)
            i = iou_score(preds, masks)

            dice_scores.append(d.item())
            iou_scores.append(i.item())

    return np.mean(dice_scores), np.mean(iou_scores)

EPOCHS = 25
best_dice = 0.0

for epoch in range(EPOCHS):
    train_loss = train_one_epoch(model, train_loader, optimizer, loss_fn)
    val_dice, val_iou = evaluate(model, test_loader)

    print(f"Epoch [{epoch+1}/{EPOCHS}] | "
          f"Train Loss: {train_loss:.4f} | "
          f"Dice: {val_dice:.4f} | "
          f"IoU: {val_iou:.4f}")

    if val_dice > best_dice:
        best_dice = val_dice
        torch.save(model.state_dict(), "unet_baseline_brisc.pth")
        print("âœ… Best U-Net model saved")

def pixel_accuracy(preds, targets):
    correct = (preds == targets).float().sum()
    total = torch.numel(preds)
    return correct / total

def evaluate(model, loader):
    model.eval()
    dice_scores = []
    iou_scores = []
    acc_scores = []

    with torch.no_grad():
        for images, masks in loader:
            images = images.to(device)
            masks = masks.to(device)

            outputs = model(images)
            preds = torch.sigmoid(outputs)
            preds = (preds > 0.5).float()

            d = dice_score(preds, masks)
            i = iou_score(preds, masks)
            a = pixel_accuracy(preds, masks)

            dice_scores.append(d.item())
            iou_scores.append(i.item())
            acc_scores.append(a.item())

    return (
        np.mean(dice_scores),
        np.mean(iou_scores),
        np.mean(acc_scores)
    )

val_dice, val_iou, val_acc = evaluate(model, test_loader)

print(f"Epoch [{epoch+1}/{EPOCHS}] | "
      f"Train Loss: {train_loss:.4f} | "
      f"Dice: {val_dice:.4f} | "
      f"IoU: {val_iou:.4f} | "
      f"Accuracy: {val_acc:.4f}")







!pip install -q timm

import torch
import torch.nn as nn
import torch.nn.functional as F
import timm

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

class ConvNeXtEncoder(nn.Module):
    def __init__(self, name="convnext_tiny", pretrained=True):
        super().__init__()
        self.backbone = timm.create_model(
            name,
            pretrained=pretrained,
            features_only=True
        )
        self.channels = self.backbone.feature_info.channels()

    def forward(self, x):
        return self.backbone(x)  # multi-scale features

class DecoderBlock(nn.Module):
    def __init__(self, in_ch, skip_ch, out_ch):
        super().__init__()
        self.conv1 = nn.Conv2d(in_ch + skip_ch, out_ch, 3, padding=1)
        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)
        self.relu = nn.ReLU(inplace=True)

    def forward(self, x, skip):
        x = F.interpolate(x, scale_factor=2, mode="bilinear", align_corners=False)
        x = torch.cat([x, skip], dim=1)
        x = self.relu(self.conv1(x))
        x = self.relu(self.conv2(x))
        return x

class ConvNeXtUNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.encoder = ConvNeXtEncoder("convnext_tiny", pretrained=True)

        # ConvNeXt feature channels
        ch = self.encoder.channels
        # Example: [96, 192, 384, 768]

        # Bottleneck
        self.bottleneck = nn.Conv2d(ch[-1], 512, kernel_size=3, padding=1)

        # Decoder (FULL depth)
        self.dec4 = DecoderBlock(512, ch[-2], 256)   # 8Ã—8 â†’ 16Ã—16
        self.dec3 = DecoderBlock(256, ch[-3], 128)   # 16Ã—16 â†’ 32Ã—32
        self.dec2 = DecoderBlock(128, ch[-4], 64)    # 32Ã—32 â†’ 64Ã—64
        self.dec1 = DecoderBlock(64, 0, 32)          # 64Ã—64 â†’ 128Ã—128

        # Final upsampling to 256Ã—256
        self.final_up = nn.Sequential(
            nn.Upsample(scale_factor=2, mode="bilinear", align_corners=False),
            nn.Conv2d(32, 16, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(16, 1, kernel_size=1)
        )

    def forward(self, x):
        f1, f2, f3, f4 = self.encoder(x)

        x = self.bottleneck(f4)
        x = self.dec4(x, f3)
        x = self.dec3(x, f2)
        x = self.dec2(x, f1)

        # No skip for last block
        x = F.interpolate(x, scale_factor=2, mode="bilinear", align_corners=False)
        x = self.dec1.conv1(torch.cat([x], dim=1))
        x = self.dec1.relu(x)
        x = self.dec1.conv2(x)
        x = self.dec1.relu(x)

        return self.final_up(x)

model = ConvNeXtUNet().to(device)

def count_params(m):
    return sum(p.numel() for p in m.parameters() if p.requires_grad) / 1e6

print(f"ConvNeXt-UNet parameters: {count_params(model):.2f} M")

loss_fn = nn.BCEWithLogitsLoss()

optimizer = torch.optim.AdamW(
    model.parameters(),
    lr=1e-4,
    weight_decay=1e-5
)

EPOCHS = 25
best_dice = 0.0

for epoch in range(EPOCHS):
    train_loss = train_one_epoch(
        model,
        train_loader,
        optimizer,
        loss_fn
    )

    dice, iou, acc = evaluate(model, test_loader)

    print(f"[ConvNeXt-UNet] Epoch {epoch+1}/{EPOCHS} | "
          f"Loss: {train_loss:.4f} | "
          f"Dice: {dice:.4f} | "
          f"IoU: {iou:.4f} | "
          f"Acc: {acc:.4f}")

    if dice > best_dice:
        best_dice = dice
        torch.save(model.state_dict(), "convnext_unet_brisc.pth")
        print("âœ… Best ConvNeXt-UNet saved")



!pip install -q timm

import torch
import torch.nn as nn
import torch.nn.functional as F
import timm
import numpy as np
from tqdm import tqdm

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

from torch.utils.data import DataLoader

BATCH_SIZE = 8
NUM_WORKERS = 2

train_loader = DataLoader(
    train_dataset,
    batch_size=BATCH_SIZE,
    shuffle=True,
    num_workers=NUM_WORKERS,
    pin_memory=True
)

test_loader = DataLoader(
    test_dataset,
    batch_size=BATCH_SIZE,
    shuffle=False,
    num_workers=NUM_WORKERS,
    pin_memory=True
)

print("âœ… train_loader and test_loader created")

class SwinEncoder(nn.Module):
    def __init__(self, pretrained=True):
        super().__init__()
        self.backbone = timm.create_model(
            "swin_tiny_patch4_window7_224",
            pretrained=pretrained,
            img_size=256,
            features_only=True
        )
        self.channels = self.backbone.feature_info.channels()

    def forward(self, x):
        feats = self.backbone(x)

        # ðŸ”¥ CRITICAL FIX: NHWC â†’ NCHW
        feats = [f.permute(0, 3, 1, 2).contiguous() for f in feats]

        return feats

class DecoderBlock(nn.Module):
    def __init__(self, in_ch, skip_ch, out_ch):
        super().__init__()
        self.conv1 = nn.Conv2d(in_ch + skip_ch, out_ch, 3, padding=1)
        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)
        self.relu = nn.ReLU(inplace=True)

    def forward(self, x, skip):
        x = F.interpolate(x, scale_factor=2, mode="bilinear", align_corners=False)
        x = torch.cat([x, skip], dim=1)
        x = self.relu(self.conv1(x))
        x = self.relu(self.conv2(x))
        return x

class DecoderBlock(nn.Module):
    def __init__(self, in_ch, skip_ch, out_ch):
        super().__init__()
        self.conv1 = nn.Conv2d(in_ch + skip_ch, out_ch, 3, padding=1)
        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)
        self.relu = nn.ReLU(inplace=True)

    def forward(self, x, skip):
        x = F.interpolate(x, scale_factor=2, mode="bilinear", align_corners=False)
        x = torch.cat([x, skip], dim=1)
        x = self.relu(self.conv1(x))
        x = self.relu(self.conv2(x))
        return x

class SwinUNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.encoder = SwinEncoder()

        ch = self.encoder.channels  # [96, 192, 384, 768]

        self.bottleneck = nn.Conv2d(ch[-1], 512, 3, padding=1)

        self.dec4 = DecoderBlock(512, ch[-2], 256)
        self.dec3 = DecoderBlock(256, ch[-3], 128)
        self.dec2 = DecoderBlock(128, ch[-4], 64)

        self.final_up = nn.Sequential(
            nn.Upsample(scale_factor=4, mode="bilinear", align_corners=False),
            nn.Conv2d(64, 32, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, 1, kernel_size=1)
        )

    def forward(self, x):
        f1, f2, f3, f4 = self.encoder(x)

        x = self.bottleneck(f4)
        x = self.dec4(x, f3)
        x = self.dec3(x, f2)
        x = self.dec2(x, f1)

        return self.final_up(x)

model = SwinUNet().to(device)

def count_params(m):
    return sum(p.numel() for p in m.parameters() if p.requires_grad) / 1e6

print(f"Swin-UNet parameters: {count_params(model):.2f} M")

loss_fn = nn.BCEWithLogitsLoss()

optimizer = torch.optim.AdamW(
    model.parameters(),
    lr=1e-4,
    weight_decay=1e-5
)

def train_one_epoch(model, loader, optimizer, loss_fn):
    model.train()
    total_loss = 0.0

    for imgs, masks in loader:
        imgs = imgs.to(device)
        masks = masks.to(device)

        optimizer.zero_grad()
        outputs = model(imgs)
        loss = loss_fn(outputs, masks)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    return total_loss / len(loader)


def evaluate(model, loader):
    model.eval()
    dices, ious, accs = [], [], []

    with torch.no_grad():
        for imgs, masks in loader:
            imgs = imgs.to(device)
            masks = masks.to(device)

            preds = torch.sigmoid(model(imgs))
            preds = (preds > 0.5).float()

            dices.append(dice_score(preds, masks).item())
            ious.append(iou_score(preds, masks).item())
            accs.append(pixel_accuracy(preds, masks).item())

    return np.mean(dices), np.mean(ious), np.mean(accs)

EPOCHS = 25
best_dice = 0.0

for epoch in range(EPOCHS):
    train_loss = train_one_epoch(
        model,
        train_loader,
        optimizer,
        loss_fn
    )

    dice, iou, acc = evaluate(model, test_loader)

    print(f"[Swin-UNet] Epoch {epoch+1}/{EPOCHS} | "
          f"Loss: {train_loss:.4f} | "
          f"Dice: {dice:.4f} | "
          f"IoU: {iou:.4f} | "
          f"Acc: {acc:.4f}")

    if dice > best_dice:
        best_dice = dice
        torch.save(model.state_dict(), "swin_unet_brisc.pth")
        print("âœ… Best Swin-UNet saved")





import torch
import torch.nn as nn
import torch.nn.functional as F
import timm

class ConvNeXtEncoder(nn.Module):
    def __init__(self, pretrained=True):
        super().__init__()
        self.backbone = timm.create_model(
            "convnext_tiny",
            pretrained=pretrained,
            features_only=True
        )
        self.channels = self.backbone.feature_info.channels()  # [96,192,384,768]

    def forward(self, x):
        return self.backbone(x)

class SwinEncoder(nn.Module):
    def __init__(self, pretrained=True):
        super().__init__()
        self.backbone = timm.create_model(
            "swin_tiny_patch4_window7_224",
            pretrained=pretrained,
            img_size=256,
            features_only=True
        )
        self.channels = self.backbone.feature_info.channels()  # [96,192,384,768]

    def forward(self, x):
        feats = self.backbone(x)
        # ðŸ”¥ Swin outputs NHWC â†’ convert to NCHW
        feats = [f.permute(0, 3, 1, 2).contiguous() for f in feats]
        return feats

class DecoderBlock(nn.Module):
    def __init__(self, in_ch, skip_ch, out_ch):
        super().__init__()
        self.conv1 = nn.Conv2d(in_ch + skip_ch, out_ch, 3, padding=1)
        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)
        self.relu = nn.ReLU(inplace=True)

    def forward(self, x, skip):
        x = F.interpolate(x, scale_factor=2, mode="bilinear", align_corners=False)
        x = torch.cat([x, skip], dim=1)
        x = self.relu(self.conv1(x))
        x = self.relu(self.conv2(x))
        return x

class ConvNeXtSwinUNet(nn.Module):
    def __init__(self):
        super().__init__()

        self.convnext = ConvNeXtEncoder(pretrained=True)
        self.swin = SwinEncoder(pretrained=True)

        ch_c = self.convnext.channels   # [96,192,384,768]
        ch_s = self.swin.channels       # [96,192,384,768]

        # Bottleneck (fused deepest features)
        self.bottleneck = nn.Conv2d(
            ch_c[-1] + ch_s[-1], 512, kernel_size=3, padding=1
        )

        # Decoder with fused skip connections
        self.dec4 = DecoderBlock(512, ch_c[-2] + ch_s[-2], 256)
        self.dec3 = DecoderBlock(256, ch_c[-3] + ch_s[-3], 128)
        self.dec2 = DecoderBlock(128, ch_c[-4] + ch_s[-4], 64)

        # Final upsampling to 256Ã—256
        self.final_up = nn.Sequential(
            nn.Upsample(scale_factor=4, mode="bilinear", align_corners=False),
            nn.Conv2d(64, 32, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, 1, kernel_size=1)
        )

    def forward(self, x):
        c_feats = self.convnext(x)
        s_feats = self.swin(x)

        # Fuse features at each scale
        f1 = torch.cat([c_feats[0], s_feats[0]], dim=1)  # 64Ã—64
        f2 = torch.cat([c_feats[1], s_feats[1]], dim=1)  # 32Ã—32
        f3 = torch.cat([c_feats[2], s_feats[2]], dim=1)  # 16Ã—16
        f4 = torch.cat([c_feats[3], s_feats[3]], dim=1)  # 8Ã—8

        x = self.bottleneck(f4)
        x = self.dec4(x, f3)
        x = self.dec3(x, f2)
        x = self.dec2(x, f1)

        return self.final_up(x)

model = ConvNeXtSwinUNet().to(device)

def count_params(m):
    return sum(p.numel() for p in m.parameters() if p.requires_grad) / 1e6

print(f"Hybrid model parameters: {count_params(model):.2f} M")

imgs, _ = next(iter(train_loader))
imgs = imgs.to(device)

with torch.no_grad():
    out = model(imgs)

print("Output shape:", out.shape)

loss_fn = nn.BCEWithLogitsLoss()

optimizer = torch.optim.AdamW(
    model.parameters(),
    lr=1e-4,
    weight_decay=1e-5
)

EPOCHS = 25
best_dice = 0.0

for epoch in range(EPOCHS):
    train_loss = train_one_epoch(model, train_loader, optimizer, loss_fn)
    dice, iou, acc = evaluate(model, test_loader)

    print(f"[Hybrid] Epoch {epoch+1}/{EPOCHS} | "
          f"Loss: {train_loss:.4f} | "
          f"Dice: {dice:.4f} | "
          f"IoU: {iou:.4f} | "
          f"Acc: {acc:.4f}")

    if dice > best_dice:
        best_dice = dice
        torch.save(model.state_dict(), "convnext_swin_unet_brisc.pth")
        print("âœ… Best Hybrid model saved")





